---
title: "Analysis 400-500 ms time window"
author: "Am√©lie la Roi"
date: "May 8, 2020"
output: html_document
---

Last update: September 2, 2020


# Data analysis on time window 400-500 ms

## Load packages
```{r warning=FALSE}
library(plyr)
library(dplyr)
library(lme4)
library(lmerTest)
library(emmeans)
library(multcomp)
library(effects)
library(ggplot2)
library(patchwork)
library(gridExtra)
library(grid)
library(pBrackets)
library(cowplot)
```


## Load data
```{r} 
# Load data study 3
print(load("G:/Back-up Y-schijf 12.03.2020/Study 3 - Follow-up MA thesis/Data/EEG/Preprocessing data/data_400-500_v3/S3.400-500_agg_v3.rdat"))

# Load data study 0
print(load("G:/Back-up Y-schijf 12.03.2020/Study 3 - Follow-up MA thesis/Data/EEG/Preprocessing data/data_400-500/400-500_agg.rdat"))

# Check variable names in dataframes
names(S3ALL.T3_agg)
names(all.T3_agg)

# Merge data study 0 and study 3
ALL2.T3 <- rbind(S3ALL.T3_agg, all.T3_agg) # 112880 + 338280 = 451160 observations --> correct!

# Check data
str(ALL2.T3) # Subject has 66 levels --> correct (50 (T1) + 16 (T2))

```

### Add session variable
```{r}
ALL2.T3$Session[ALL2.T3$Subject %in% c("1","5","9","10","12","15","16",
                                  "20","23","25","26","28","29","31",
                                  "34","36","40","41","46","47","49",
                                  "51","54","55","57","3","8","11","13",
                                  "14","17","18","21","22","24","27","33",
                                  "35","37","38","39","42","44","45","48",
                                  "50","52","53","56","58")] <- "T1"

ALL2.T3$Session[ALL2.T3$Subject %in% c("IAS30005_AR","IAS30009_AR","IAS30010_AR","IAS30016_AR","IAS30020_AR",
              "IAS30026_AR","IAS30029_AR","IAS30031_AR","IAS30034_AR","IAS30036_AR",
              "IAS30040_AR","IAS30041_AR","IAS30049_AR","IAS30051_AR",
              "IAS30054_AR","IAS30057_AR")] <- "T2"

ALL2.T3$Session <- as.factor(ALL2.T3$Session)

```


### Tidy variables
```{r results='hide'}
# Rename subject variable study 3
ALL2.T3$Subject <- revalue(ALL2.T3$Subject, c("IAS30005_AR"="5","IAS30009_AR"="9","IAS30010_AR"="10","IAS30016_AR"="16",
                         "IAS30020_AR"="20","IAS30026_AR"="26","IAS30029_AR"="29","IAS30031_AR"="31",
                         "IAS30034_AR"="34","IAS30036_AR"="36","IAS30040_AR"="40","IAS30041_AR"="41",
                         "IAS30049_AR"="49","IAS30051_AR"="51","IAS30054_AR"="54", "IAS30057_AR"="57"))

#check data
str(ALL2.T3) # Subject variable has 50 levels --> correct

# Only select data older adults
ALL3.T3 <- droplevels(ALL2.T3[ALL2.T3$Group == 'senior',])

# Check data
str(ALL3.T3) # Subject variable has 25 levels --> correct

# check dummy coding and relevel if necessary
contrasts(ALL3.T3$Session)

# check Context factor
ALL3.T3 <- rename(ALL3.T3, Context=Constraint) # rename factor to 'Context'
contrasts(ALL3.T3$Context)
ALL3.T3$Context <- relevel(ALL3.T3$Context, "Neutral")
ALL3.T3$Context <- revalue(ALL3.T3$Context, c("High"="Predictive")) # rename factor level 'High' to 'Predictive'
contrasts(ALL3.T3$Context)

# Check Idiomaticity factor
contrasts(ALL3.T3$Idiomaticity)
ALL3.T3$Idiomaticity <- relevel(ALL3.T3$Idiomaticity, "Lit")

# Check Correctness factor
ALL3.T3 <- rename(ALL3.T3, Correctness=Condition)
contrasts(ALL3.T3$Correctness)

# Check dataframe
head(ALL3.T3)
summary(ALL3.T3)
str(ALL3.T3)
```

# Create dataset for analysis
```{r}
# Compute mean voltage over session, group, the 8 experimental conditions, and the 4 ROIs
ALL3.T3_agg <- ddply(ALL3.T3, c("Subject", "Session", "Group", "Context", "Idiomaticity", "Correctness", 
                                "TrialNr", "Target", "Anteriority", "Hemisphere", "ROI"), summarise, 
                                mean.mV = mean(mV, na.rm=TRUE),
                                subj.n = length(mV[!is.na(mV)]))

str(ALL3.T3_agg)

# Only select posterior ROIs
ALL3.T3_post1 <- droplevels(ALL3.T3_agg[ALL3.T3_agg$Anteriority == 'post',])
str(ALL3.T3_post1)
summary(ALL3.T3_post1)

# Load VF data
print(load("G:/Back-up Y-schijf 12.03.2020/Study 3 - Follow-up MA thesis/Data/Cognitive tasks/Verbal fluency/S0S3_VF_final.rdat"))

# Remove redundant columns
VF_final <- subset(VF4, select=c(Subject, Session, Cat.c, Let.c, Total.c))

# Merge VF scores with EEG data
ALL3.T3_post <- merge(ALL3.T3_post1, VF_final, by=c("Subject", "Session"))
summary(ALL3.T3_post)
```

# Test for general longitudinal effects in the N400
```{r}
# Average mean voltages over Context and Idiomaticity
ALL3.T3_N400 <- ddply(ALL3.T3_post, c("Subject", "Session", "Group", "Correctness", "TrialNr", "Target", "Anteriority", "Hemisphere", "ROI"), summarise, mean.mV = mean(mean.mV, na.rm=TRUE))

# Only select posterior ROIs
T3N400_post <- droplevels(ALL3.T3_N400[ALL3.T3_N400$Anteriority=='post',])

# Fit model on general N400 effect in baseline and follow-up session
m0.T3 <- lmer(mean.mV ~ Session * Correctness + Hemisphere + (1|Subject) + (1|Target), data=T3N400_post, control=lmerControl(optimizer="Nelder_Mead"), REML=F)
summary(m0.T3) # Significant interaction between Session * Correctness, so general N400 effect seems to change over time

# Compute contrasts
(T3_contrastN400_Correctness <- lsmeans(m0.T3, pairwise ~ Correctness|Session))
(T3_contrastN400_Session <- lsmeans(m0.T3, pairwise ~ Session|Correctness))

# Change reference level of correctness to facilitate interpretation of planned comparisons on N400 effect
contrasts(T3N400_post$Correctness)
T3N400_post$Correctness <- relevel(T3N400_post$Correctness, "incorrect")

# Fit model again with relevel factor
m0a.T3 <- lmer(mean.mV ~ Session * Correctness + Hemisphere + (1|Subject) + (1|Target), data=T3N400_post, control=lmerControl(optimizer="Nelder_Mead"), REML=F)

(T3_contrastN400_Correctness_v2 <- lsmeans(m0a.T3, pairwise ~ Correctness|Session))
(T3_contrastN400_Session_v2 <- lsmeans(m0a.T3, pairwise ~ Session|Correctness))

m0b.T3 <- lmer(mean.mV ~ Session + Correctness + Hemisphere + (1|Subject) + (1|Target), data=T3N400_post, control=lmerControl(optimizer="Nelder_Mead"), REML=F)
anova(m0a.T3, m0b.T3)
AIC(m0b.T3)-AIC(m0a.T3)


```

## Plot general longitudinal effects in the N400 effect

# Plot N400 effect in barplot
```{r}

# Compute mean voltage for correct and incorrect target words in baseline and follow-up session
plot_m0.T3 <- allEffects(m0.T3)
plot_m0.T3 # print effects

# Save interaction effect of the model in a dataframe
## 300-400 ms
T3intN400 <- plot_m0.T3[[2]] # save the interaction effect
T3intN400.df <- as.data.frame(T3intN400) # convert into dataframe

# Rename factor levels Session
levels(T3intN400.df$Session)[levels(T3intN400.df$Session)=="T1"] <- "Baseline"
levels(T3intN400.df$Session)[levels(T3intN400.df$Session)=="T2"] <- "Follow-up"
contrasts(T3intN400.df$Session)

# Make barplot
(TW3_N400_bar <- ggplot(data=T3intN400.df, aes(x=Session, y=fit, fill=Correctness)) +
  geom_bar(stat="identity", position=position_dodge(), colour="black", size=1) +
  scale_fill_grey() + coord_cartesian(ylim=c(-1,4)) + scale_y_continuous(breaks=seq(-1,4,1)) +
  background_grid(major='none', minor="none") + labs(x=NULL, y="\nFitted values mV") +
  ggtitle("\n400-500 ms") + theme_classic() +
  theme(plot.title=element_text(size= 24, color="black", face="bold", hjust=0.5),
  axis.title.y=element_text(size=32, color="black", face="bold"), axis.text=element_text(size=28, color="black", face="bold"), axis.line=element_line(size=1), axis.ticks = element_line(size=1), axis.ticks.length = unit(0.3,"cm"), 
  legend.text=element_text(size=24, color="black"), legend.title=element_text(size=28, color="black", face="bold"),
  legend.position="bottom",
  plot.margin=margin(r=1, l=1, unit="cm"), panel.grid.major.y=element_blank())+
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), width=0.2, position=position_dodge(0.9)))

# Add significance stars
## Write function to add brackets and save plot
bracketsGrob <- function(...){
l <- list(...)
e <- new.env()
e$l <- l
  grid:::recordGrob(  {
    do.call(grid.brackets, l)
  }, e)
}

# Define position of brackets
b1 <- bracketsGrob(0.18, 0.85, 0.37, 0.85, h=0.02, lwd=1, col="black", type=1, ticks=NULL) # Literal incorrect - correct
b2 <- bracketsGrob(0.63, 0.85, 0.82, 0.85, h=0.02,  lwd=1, col="black", type=1, ticks=NULL) # Idiomatic incorrect - correct
b3 <- bracketsGrob(0.82, 0.15, 0.37, 0.15, h=0.02,  lwd=1, col="black", type=1, ticks=NULL) # correct literal - idiomatic


## Add brackets and significance stars to plot
(TW3_N400_bar_final <- TW3_N400_bar + 
                    annotation_custom(b1) + annotation_custom(b2) + 
                    annotation_custom(b3) + 
                    annotate("text", label="***", x=1.00, y= 3.60) + #Baseline incorrect - correct
                    annotate("text", label="***", x=2.00, y= 3.60) + # Follow-up incorrect - correct
                    annotate("text", label="***", x=1.73, y= -0.65))  # Incorrect Baseline vs. Follow-up

# Save plot
# ggsave(TW3_N400_bar_final, file="./Plots/Rplots/TW3_N400_bar_final.png", width=10, height=10, units="in")
# ggsave(TW3_N400_bar_final, file="./Plots/Rplots/TW3_N400_bar_final.pdf", width=10, height=10, units="in")
# ggsave(TW3_N400_bar_final, file="./Plots/Rplots/TW3_N400_bar_final.tiff", width=10, height=10, units="in")

```


# Model fitting
## Start with full model with by-Subject and by-Target(=item) random intercepts
```{r}
m1.T3 <- lmer(mean.mV ~ Session * Context * Idiomaticity * Correctness + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)

summary(m1.T3) # significant interaction Idiomaticity*Correctness, Session*Correctness, and Context*Idiomaticity
```

## Remove Context from 4-way interaction
```{r}
m2.T3 <- lmer(mean.mV ~ Session * Idiomaticity * Correctness + Context + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m2.T3)
AIC(m1.T3)-AIC(m2.T3) # model with 3-way interaction has lower AIC
anova(m2.T3, m1.T3) # 4-way interaction doesn't significantly improve model fit
```

## Remove Idiomaticity from 3-way interaction
```{r}
m3.T3 <- lmer(mean.mV ~ Session * Correctness + Context + Idiomaticity + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m3.T3)
AIC(m2.T3)-AIC(m3.T3) # model with 3-way interaction has lower AIC
anova(m2.T3, m3.T3) # 3-way interaction significantly improves model fit
```

## Remove Session from 3-way interaction
```{r}
m4.T3 <- lmer(mean.mV ~ Idiomaticity * Correctness + Session + Context + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m4.T3)
AIC(m2.T3)-AIC(m4.T3) # no AIC difference
anova(m2.T3, m4.T3) # 3-way interaction doesn't significantly improve model fit
```

## Add Context to 2-way interaction
```{r}
m5.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m5.T3)
AIC(m5.T3)-AIC(m4.T3) # model with 3-way interaction has lower AIC
anova(m5.T3, m4.T3) # model with 3-way interaction significantly improves model fit
```

## Remove Idiomaticity from 3-way interaction
```{r}
m6.T3 <- lmer(mean.mV ~ Context * Correctness + Session + Idiomaticity + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m6.T3)
AIC(m6.T3)-AIC(m5.T3) # model with 3-way interaction has lower AIC
anova(m6.T3, m5.T3) # 3-way interaction significantly improves model fit
```

## Remove Correctness from 3-way interaction
```{r}
m7.T3 <- lmer(mean.mV ~ Context * Idiomaticity + Correctness + Session + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m7.T3)
AIC(m7.T3)-AIC(m5.T3) # model with 3-way interaction has lower AIC
anova(m7.T3, m5.T3) # 3-way interaction significantly improves model fit
```


#### BEST MODEL SO FAR: m5.T3

## Test effect of hemisphere
```{r}
# Test effect Hemisphere
m8.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m8.T3) # Hemisphere is significant --> right more negative than left
AIC(m8.T3)-AIC(m5.T3) # model including Hemisphere has lower AIC
anova(m8.T3, m5.T3) # including Hemisphere significantly improves model fit
```


## Add random slopes
```{r}
# Add random slope for Idiomaticity per Subject
m9.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=T)
summary(m9.T3)

# Fit model with REML=T to compare to random slope model
m8a.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1|Subject) + (1|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=T)
anova(m8a.T3, m9.T3) # model with random slope is significantly better

# Add random slope for Context per Target
m10.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=T)
summary(m10.T3)

AIC(m9.T3)-AIC(m10.T3) # model with both random slopes has lower AIC
anova(m9.T3, m10.T3) # model with both random slopes is significantly better

```


### BEST MODEL SO FAR: m10.T3

## Test effect Verbal fluency
```{r}

# Test effect category fluency
m11.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + Cat.c + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)


# Refit model 10 with REML=F to enable model comparison
m10a.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)

summary(m11.T3) # Category fluency is significant
anova(m10a.T3, m11.T3) # including Category fluency significantly improves model fit

# Test effect letter fluency
m12.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + Let.c + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m12.T3)# Letter fluency is not significant
anova(m10a.T3, m12.T3) # inclusion of letter fluency doesn't sigificantly improve model fit

# Test effect total fluency
m13.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + Total.c + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post, control=lmerControl(optimizer="bobyqa"), REML=F)
summary(m13.T3)# Total VF score is not significant
anova(m10a.T3, m13.T3) # inclusion of Total VF doesn't sigificantly improve model fit
```

# BEST MODEL: m11.T3 

## Check distribution of residuals and trim model if needed
```{r}
qqnorm(resid(m11.T3))
qqline(resid(m11.T3)) # skewed distribution, some extreme outliers
plot(fitted(m11.T3), resid(m11.T3)) # some extreme outliers
hist(resid(m11.T3)) # looks pretty normal, except for some outliers in the residuals

## trim data
ALL3.T3_post.trim <- ALL3.T3_post[abs(scale(resid(m11.T3))) < 2.5, ]

## save trimmed data
save(ALL3.T3_post.trim, file="./EEG - Final data/ALL3.T3_post.trim_v3.rdat" )

## fit model with trimmed data
m11a.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + Cat.c + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post.trim, control=lmerControl(optimizer="Nelder_Mead"), REML=T)
summary(m11a.T3) # Category fluency is not significant anymore; 3-way interaction is reduced to trend (p=.05)

# Remove Category fluency again
m11b.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post.trim, control=lmerControl(optimizer="Nelder_Mead"), REML=F)
summary(m11b.T3)

# Fit m11a.T3 with REML=F to enable model comparison
m11c.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + Cat.c + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post.trim, control=lmerControl(optimizer="Nelder_Mead"), REML=F)

anova(m11b.T3, m11c.T3) # inclusion of Category fluency doesn't improve model fit when models are fitted with trimmed data

### BEST MODEL: m11b.T3 --> fit with REML=T for model coefficients
m11d.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post.trim, control=lmerControl(optimizer="nlminbwrap"), REML=T)
summary(m11d.T3) # 3-way interaction is only trend now (p=.051)

## check how much data was removed
## Compare to m10.T3, because Category fluency needs to be excluded
(noutliers <- sum(abs(scale(resid(m10.T3))) >= 2.5))
noutliers/nrow(ALL3.T3_post) # only 1.6 % of the data was removed

## check change in correlation between observed and fitted values after trimming the data
cor(ALL3.T3_post$mean.mV, fitted(m10.T3))^2 # is 0.126
cor(ALL3.T3_post.trim$mean.mV, fitted(m11d.T3))^2 # increased to 0.137

## check distribution of residuals after trimming
qqnorm(resid(m11d.T3))
qqline(resid(m11d.T3)) # better distribution
plot(fitted(m11d.T3), resid(m11d.T3)) # better
hist(resid(m11d.T3)) # looks normal
```

# Final model: m11d.T3 (for summmary of coefficients final model set REML to TRUE)

## To get insight in the direction of the interaction between Context x Idiomaticity x Correctness, calculate contrasts
```{r}
# Calculate contrasts for 3-way interaction
(T3_contrast_Correctness <- lsmeans(m11d.T3, pairwise ~ Correctness|Idiomaticity|Context)) # Incorrect significantly more negative than correct in Literal sentences predicted by Neutral and Predictive context. No difference incorrect - correct for idioms, for neither context condition

(T3_contrast_Idiomaticity <- lsmeans(m11d.T3, pairwise ~ Idiomaticity|Correctness|Context)) # Idiom more positive than Literal in correct and incorrect sentences preceded by neutral context, and in incorrect sentences predicted by predictive context --> correct literal sentences preceded by predictive context are facilitated to such a degree that it resembles the facilitated processing of idioms.

(T3_contrast_Context <- lsmeans(m11d.T3, pairwise ~ Context|Idiomaticity|Correctness)) # Predictive significantly less negative than Neutral only for correct target words in Literal sentences --> discomfirmed predictions are actually hampered by a predictive context, because the violation is stronger. Context does not add to the retrieval of idioms, because this is already facilitated. 

# visualize 3-way interaction
lsmip(m11d.T3, Idiomaticity ~ Correctness|Context) # Context helps prediction in Literal sentences --> smaller negativity
lsmip(m11d.T3, Correctness ~ Idiomaticity|Context) # Predictive context decreases negativity of correct literal sentences to such a degree that it reaches the voltage level of idioms
lsmip(m11d.T3, Context ~ Correctness|Idiomaticity) # Only context effect is on correct literal sentences
# Context x Idiomaticity
(T3_contrast_Context <- lsmeans(m11d.T3, pairwise ~ Context|Idiomaticity)) # No difference Neutral vs. Predictive in Lit nor Idiom, but trend in Lit with Predictive less negative than Neutral (p=.09)
(T3_contrast_Idiomaticity <- lsmeans(m11d.T3, pairwise ~ Idiomaticity|Context)) # Idiom more positive than Lit in both Predictive and Neutral, but difference bigger in Neutral


```


## Change reference level Correctness to make contrast easier to interpret
```{r}
contrasts(ALL3.T3_post.trim$Correctness)
ALL3.T3_post.trim$Correctness <- relevel(ALL3.T3_post.trim$Correctness, "incorrect")
contrasts(ALL3.T3_post.trim$Correctness)

# Fit model again with releveled factor
m11e.T3 <- lmer(mean.mV ~ Context * Idiomaticity * Correctness + Session + Hemisphere + (1+Idiomaticity|Subject) + (1+Context|Target), data=ALL3.T3_post.trim, control=lmerControl(optimizer="nlminbwrap"), REML=T)

# Calculate contrasts again
(T3_contrast_Correctness2 <- lsmeans(m11e.T3, pairwise ~ Correctness|Idiomaticity|Context)) 
(T3_contrast_Idiomaticity2 <- lsmeans(m11e.T3, pairwise ~ Idiomaticity|Correctness|Context)) 
(T3_contrast_Context2 <- lsmeans(m11e.T3, pairwise ~ Context|Idiomaticity|Correctness))

# Change back reference level to correct
ALL3.T3_post.trim$Correctness <- relevel(ALL3.T3_post.trim$Correctness, "correct")
contrasts(ALL3.T3_post.trim$Correctness)

```

### Save final models
```{r}

save(m11d.T3, file="./EEG - Final data/m11d.T3.rdat" )
save(m11e.T3, file="./EEG - Final data/m11e.T3.rdat" )
```


## Present interaction Context x Idiomaticity x Correctness in barplot
```{r, fig.height=5, fig.width=10}
# Save effects for the model

## 400-500 ms
plot_m11d.T3 <- allEffects(m11d.T3)
plot_m11d.T3 # print effects

# Save interaction effect of the model in a dataframe
## 300-400 ms
T3int <- plot_m11d.T3[[3]] # save the interaction effect
T3int.df <- as.data.frame(T3int) # convert into dataframe
T3int.df <- droplevels(T3int.df)

# Check reference levels
# Factor Context
contrasts(T3int.df$Context) # Neutral is reference level --> correct

# Factor Idiomaticity
contrasts(T3int.df$Idiomaticity) # Idiom is reference level --> relevel
T3int.df$Idiomaticity <- factor(T3int.df$Idiomaticity, levels=c("Lit", "Idiom"))
contrasts(T3int.df$Idiomaticity) # Now Lit is reference level --> correct

# Rename factor levels Idiomaticity
levels(T3int.df$Idiomaticity)[levels(T3int.df$Idiomaticity)=="Lit"] <- "Literal"
levels(T3int.df$Idiomaticity)[levels(T3int.df$Idiomaticity)=="Idiom"] <- "Idiomatic"
contrasts(T3int.df$Idiomaticity)

# Factor Correctness
contrasts(T3int.df$Correctness) # correct is reference level

# Print range mV
range(T3int.df$fit) # 0.09 till 1.98


## Plot interaction Context * Idiomaticity * Correctness
### Literal sentences
(TW3_Lit_bar <- ggplot(data=T3int.df[T3int.df$Idiomaticity=="Literal",], aes(x=Context, y=fit, fill=Correctness)) +
  geom_bar(stat="identity", position=position_dodge(), linetype=0) +
  scale_fill_grey() + coord_cartesian(ylim=c(-1,4)) + scale_y_continuous(breaks=seq(-1,4,1)) +
  background_grid(major='y', minor="none") + labs(x=NULL, y="\nFitted values mV") +
  ggtitle("400-500 ms\nLiteral sentences") + theme_classic() +
  theme(plot.title=element_text(size= 16, color="black", face="bold", hjust=0.5), axis.title.y=element_text(size=16, color="black", face="bold"), axis.text=element_text(size=16, color="black", face="bold"), legend.text=element_text(size=14, color="black"),
legend.title=element_text(size=16, color="black", face="bold"), plot.margin=margin(r=1, l=1, unit="cm"), panel.grid.major.y=element_line(colour="lightgrey", size=0.2))+
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), width=0.2, position=position_dodge(0.9)))

### Predictive context
(TW3_Idiom_bar <- ggplot(data=T3int.df[T3int.df$Idiomaticity=="Idiomatic",], aes(x=Context, y=fit, fill=Correctness)) +
  geom_bar(stat="identity", position=position_dodge(), linetype=0) +
  scale_fill_grey() + coord_cartesian(ylim=c(-1,4)) + scale_y_continuous(breaks=seq(-1,4,1)) +
  background_grid(major='y', minor="none") + labs(x=NULL, y="\nFitted values mV") +
  ggtitle("400-500 ms\nIdiomatic sentences") + theme_classic() +
  theme(plot.title=element_text(size= 16, color="black", face="bold", hjust=0.5), axis.title.y=element_text(size=16, color="black", face="bold"), axis.text=element_text(size=16, color="black", face="bold"), legend.text=element_text(size=14, color="black"),
legend.title=element_text(size=16, color="black", face="bold"), plot.margin=margin(r=1, l=1, unit="cm"), panel.grid.major.y=element_line(colour="lightgrey", size=0.2))+
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), width=0.2, position=position_dodge(0.9)))

# Combine plots
## Delete legend
TW3_Lit_bar_nolegend <- TW3_Lit_bar + theme(legend.position = "none")
TW3_Idiom_bar_nolegend <- TW3_Idiom_bar + theme(legend.position = "none")

# Save legend
legendN400TW3 <- get_legend(TW3_Lit_bar + theme(legend.position = "bottom", legend.justification = "top"))

# Make one plot
(intN400_TW3 <- grid.arrange(TW3_Lit_bar_nolegend, TW3_Idiom_bar_nolegend,
                                        ncol=2, top=textGrob("400-500 ms\n",
                                        gp=gpar(fontsize=16, fontface="bold")), bottom=legendN400TW3))

# Add significance stars
## Write function to add brackets and save plot
bracketsGrob <- function(...){
l <- list(...)
e <- new.env()
e$l <- l
  grid:::recordGrob(  {
    do.call(grid.brackets, l)
  }, e)
}

# Define position of brackets
## Literal sentences
b1 <- bracketsGrob(0.18, 0.85, 0.37, 0.85, h=0.02, lwd=1, col="black", type=1, ticks=NULL) # Literal incorrect - correct
b2 <- bracketsGrob(0.63, 0.85, 0.82, 0.85, h=0.02,  lwd=1, col="black", type=1, ticks=NULL) # Idiomatic incorrect - correct
b3 <- bracketsGrob(0.63, 0.15, 0.18, 0.15, h=0.02,  lwd=1, col="black", type=1, ticks=NULL) # correct literal - idiomatic


## Add brackets and significance stars to plot
(intN400_TW3_Lit <- TW3_Lit_bar + 
                    annotation_custom(b1) + annotation_custom(b2) + 
                    annotation_custom(b3) + 
                    annotate("text", label="***", x=1.00, y= 3.60) + # Neut Lit incorrect - correct
                    annotate("text", label="***", x=2.00, y= 3.60) + # Pred Lit incorrect - correct
                    annotate("text", label="**", x=1.30, y= -0.65))  # Lit Correct Pred-Neut

## Idiomatic sentences --> no significant differences, so no asterisks needed

## Combine plots literal and idiomatic sentences
(TW3_ContIdiomCor_bar <- cowplot::plot_grid(intN400_TW3_Lit + theme(legend.position="bottom"),
                            TW3_Idiom_bar + theme(legend.position="bottom"),
                            ncol=2, nrow=1, align='vh'))
  
# Save plot
# ggsave(TW3_ContIdiomCor_bar, file="./Plots/Rplots/TW3_ContIdiomCor_bar.png", width=10, height=5, units="in")
# ggsave(TW3_ContIdiomCor_bar, file="./Plots/Rplots/TW3_ContIdiomCor_bar.pdf", width=10, height=5, units="in")
# ggsave(TW3_ContIdiomCor_bar, file="./Plots/Rplots/TW3_ContIdiomCor_bar.tiff", width=10, height=5, units="in")
```
